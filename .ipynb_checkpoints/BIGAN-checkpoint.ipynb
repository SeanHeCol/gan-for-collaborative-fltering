{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import os\n",
    "import bottleneck as bn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bigan:\n",
    "    def __init__(self, p_dims, d_dims,q_dims=None,random_seed=98765,g_weight = 0.5,lr=1e-3):\n",
    "        \n",
    "        self.p_dims = p_dims\n",
    "        self.d_dims = d_dims\n",
    "        self.random_seed = random_seed\n",
    "        self.lr = lr\n",
    "        self.g_weight = g_weight\n",
    "        if q_dims is None:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "        else:\n",
    "            assert q_dims[0] == p_dims[-1], \"Input and output dimension must equal each other for autoencoders.\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q-network mismatches.\"\n",
    "            \n",
    "        assert d_dims is not None, \"d_dims can't be None\"\n",
    "        assert d_dims[0] == self.p_dims[0] + self.q_dims[0], \"Shape mismatch: discriminate network\\\n",
    "        should be equal to the sum of input shape and output shape of p.\"\n",
    "            \n",
    "        self._construct_weights()\n",
    "        self.construct_placeholders()\n",
    "    def construct_placeholders(self):        \n",
    "        self.input_X = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.q_dims[0]])\n",
    "        self.input_z = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.p_dims[0]])\n",
    "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=None)\n",
    "        self.is_training_ph = tf.placeholder_with_default(0.0, shape=None)\n",
    "    \n",
    "    def _construct_weights(self):\n",
    "        self.weights_q, self.biases_q = [], []\n",
    "        \n",
    "        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1], self.q_dims[1:])):\n",
    "            weight_key = \"weight_q_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_q_{}\".format(i+1)\n",
    "            \n",
    "            self.weights_q.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_q.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # add summary stats\n",
    "            #tf.summary.histogram(weight_key, self.weights_q[-1])\n",
    "            #tf.summary.histogram(bias_key, self.biases_q[-1])\n",
    "            \n",
    "        self.weights_p, self.biases_p = [], []\n",
    "        \n",
    "        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-1], self.p_dims[1:])):\n",
    "            weight_key = \"weight_p_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_p_{}\".format(i+1)\n",
    "            self.weights_p.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_p.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "        self.weights_d,self.biases_d = [],[]\n",
    "        \n",
    "        for i, (d_in, d_out) in enumerate(zip(self.d_dims[:-1], self.d_dims[1:])):\n",
    "            weight_key = \"weight_d_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_d_{}\".format(i+1)\n",
    "            self.weights_d.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_d.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "        weight_key = \"weight_d_out\"\n",
    "        bias_key = \"bias_d_out\"\n",
    "        self.weights_d.append(tf.get_variable(\n",
    "                name=weight_key, shape=[self.d_dims[-1], 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "        self.biases_d.append(tf.get_variable(\n",
    "                name=bias_key, shape=[1],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "    def p_graph(self, z):\n",
    "        h = z\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_p, self.biases_p)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights_p) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return h\n",
    "    def q_graph(self,x):\n",
    "        \n",
    "        h = tf.nn.l2_normalize(x,1)\n",
    "        \n",
    "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_q, self.biases_q)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            h = tf.nn.tanh(h)\n",
    "        return h\n",
    "    def forward(self,x):\n",
    "        z = self.q_graph(x)\n",
    "        x_hat = self.p_graph(z)\n",
    "        return x_hat\n",
    "    def d_graph(self,x,z):\n",
    "        concat = tf.concat([x,z],axis=1)\n",
    "        h = concat\n",
    "        # adding the random noise to make it harder for disriminator to do its job\n",
    "        #randNoise = tf.random_normal(tf.shape(h)) \n",
    "        #h += randNoise\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_d, self.biases_d)):\n",
    "            if i != len(self.weights_d):\n",
    "                h = tf.matmul(h, w) + b\n",
    "                h = tf.nn.tanh(h)\n",
    "            else:\n",
    "                h = tf.matmul(h,w) + b\n",
    "        logits = h\n",
    "        #return tf.nn.softmax(logits,axis=1)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def cost(self,x,z_hat,z,x_hat):\n",
    "        \n",
    "        # critic for encoder\n",
    "        \n",
    "        # adding some noise to real x or there will be a lot of 0s, which is totally different from x_hat\n",
    "       \n",
    "        \n",
    "        \n",
    "        #x = x + noise\n",
    "        \n",
    "        #x_hat = tf.nn.dropout(x_hat,10)\n",
    "        \n",
    "        #x = tf.nn.l2_normalize(x, 1)\n",
    "        \n",
    "        pred_q = self.d_graph(x,z_hat)\n",
    "        \n",
    "        # critic for decoder\n",
    "        pred_p = self.d_graph(x_hat,z)\n",
    "        \n",
    "        #D(x,E(x))\n",
    "        sig_q = tf.nn.sigmoid(pred_q)\n",
    "        \n",
    "        #D(G(z),z)\n",
    "        sig_p = tf.nn.sigmoid(pred_p)\n",
    "        \n",
    "        # loss for D is decode - encoder this value should be negative in this case minimizing the negative value\n",
    "        # \n",
    "        \n",
    "        real = tf.reduce_mean(pred_q)\n",
    "        fake = tf.reduce_mean(pred_p)\n",
    "        \n",
    "        #loss_d = tf.reduce_mean(pred_p - pred_q)\n",
    "        \n",
    "        loss_d = fake - real\n",
    "        \n",
    "        # loss for G and E \n",
    "        #loss_eg = tf.reduce_mean(pred_q - pred_p)\n",
    "        \n",
    "        loss_eg = real - fake\n",
    "        \n",
    "        para_eg = self.weights_q + self.weights_p + self.biases_q + self.biases_p\n",
    "        para_d = self.weights_d + self.biases_d\n",
    "        \n",
    "        EG_solver = (tf.train.RMSPropOptimizer(learning_rate=self.lr)\n",
    "            .minimize(loss_eg, var_list=para_eg))\n",
    "        D_solver = (tf.train.RMSPropOptimizer(learning_rate=self.lr)\n",
    "            .minimize(loss_d, var_list=para_d))\n",
    "        \n",
    "        clip_D = [p.assign(tf.clip_by_value(p, -0.02, 0.02)) for p in para_d]\n",
    "        #tf.summary.scalar(\"loss1\",loss1)\n",
    "        #tf.summary.scalar(\"loss2\",loss2)\n",
    "        #tf.summary.scalar(\"loss3\",loss3)\n",
    "        #tf.summary.scalar(\"loss4\",loss4)\n",
    "        tf.summary.scalar(\"loss_eg\",loss_eg)\n",
    "        tf.summary.scalar(\"loss_d\", loss_d)\n",
    "        tf.summary.scalar(\"fake_mean\", fake)\n",
    "        tf.summary.scalar(\"real_mean\", real)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return loss_eg, loss_d, EG_solver, D_solver, clip_D, merged\n",
    "    \n",
    "    \n",
    "    def build_graph(self):\n",
    "        z_hat = self.q_graph(self.input_X)\n",
    "        z = tf.random_normal(tf.shape(z_hat))\n",
    "        x_hat = self.p_graph(self.is_training_ph * z + (1- self.is_training_ph) * z_hat)\n",
    "        loss_eg, loss_d, EG_solver, D_solver, clip_D ,merged = self.cost(self.input_X,z_hat,z,x_hat)\n",
    "        saver = tf.train.Saver()\n",
    "        return saver,x_hat,loss_eg, loss_d, EG_solver, clip_D , D_solver, merged\n",
    "        \n",
    "    def train(self,train,val_tr = None,val_te = None,n_epochs = 200,batch_size = 500):\n",
    "        #pass\n",
    "        ndcgs_vad = []\n",
    "        global predict_vad \n",
    "        N = train.shape[0]\n",
    "        idxlist = list(range(N))\n",
    "        saver, x_hat, loss_eg, loss_d, EG_solver, clip_D ,D_solver, merged = self.build_graph()\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            training_writer = tf.summary.FileWriter('./train_summary/g_{}/'.format(self.g_weight),\n",
    "                                      sess.graph)\n",
    "            sess.run(init)\n",
    "            count = 0\n",
    "            for epoch in range(n_epochs):\n",
    "                np.random.shuffle(idxlist)\n",
    "                start_idxs = list(range(0,N,batch_size))\n",
    "                end_idxs = start_idxs[1:] + [N]\n",
    "                for batch_num in range(len(start_idxs)):\n",
    "                    start_idx = start_idxs[batch_num]\n",
    "                    end_idx = end_idxs[batch_num]\n",
    "                    \n",
    "                    X = train[idxlist[start_idx:end_idx]]\n",
    "                    \n",
    "                    if sparse.isspmatrix(X):\n",
    "                        X = X.toarray()\n",
    "                    X = X.astype(\"float32\")\n",
    "                    \n",
    "                    feed_dict = {self.input_X:X,\n",
    "                                 self.keep_prob_ph:1,\n",
    "                                 self.is_training_ph:1}\n",
    "                    \n",
    "                    for _ in range(5): \n",
    "                        d_batch = np.random.randint(len(start_idxs))\n",
    "                        D_X = train[idxlist[start_idxs[d_batch]:end_idxs[d_batch]]]\n",
    "                        if sparse.isspmatrix(D_X):\n",
    "                            D_X = D_X.toarray()\n",
    "                            \n",
    "                        D_X = D_X.astype(\"float32\")\n",
    "                        \n",
    "                        feed_D = {self.input_X:D_X,\n",
    "                                 self.keep_prob_ph:1,\n",
    "                                 self.is_training_ph:1}\n",
    "                        #print (feed_D)\n",
    "                        sess.run(D_solver,feed_dict=feed_D)\n",
    "                        sess.run(clip_D,feed_dict=feed_D)\n",
    "                    sess.run(EG_solver, feed_dict=feed_dict)\n",
    "                    if batch_num % 100 == 0:\n",
    "                        #summary_train = sess.run(merged,feed_dict = feed_dict)\n",
    "                        merged_summary = sess.run(merged, feed_dict = feed_dict)\n",
    "                        training_writer.add_summary(merged_summary, global_step=epoch*int(N/batch_size/100) + batch_num)\n",
    "                        if val_tr != None and val_te != None:\n",
    "                            val_predict = self.predict(val_tr,sess,x_hat)\n",
    "                            #predict_vad.append(val_predict)\n",
    "                            #print (val_predict)\n",
    "                            val_predict[val_tr.nonzero()] = -np.inf\n",
    "                            ndcg_dist = np.array(NDCG_binary_at_k_batch(val_predict,val_te))\n",
    "                            ndcg_val = ndcg_dist.mean()\n",
    "                            print (\"Validation of NDCG @ 100 at epoch {} is {}\".format(epoch,ndcg_val))\n",
    "                            ndcgs_vad.append(ndcg_val)\n",
    "                    count += 1\n",
    "        return ndcgs_vad          \n",
    "    def predict(self,test,sess,x_hat,batch_size = 500):\n",
    "        \n",
    "        N = test.shape[0]\n",
    "        res = []\n",
    "        for start_idx in range(0,N,batch_size):\n",
    "            end_idx = min(start_idx + batch_size, N)\n",
    "            X = test[start_idx:end_idx]\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype(\"float32\")\n",
    "            feed_dict = {self.input_X:X,\n",
    "                         self.is_training_ph:0}\n",
    "            res.append(sess.run(x_hat,feed_dict=feed_dict))\n",
    "        return np.vstack(res)\n",
    "        \n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Calculating the NDCG@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG\n",
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the training, validating, and testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "DATA_DIR = './data/ml-20m/'\n",
    "DATA_DIR = \"/home/jz2884/ADV/data/ml-20m\"\n",
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "unique_sid = list()\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_data(csv_file):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    n_users = tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_items))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = load_train_data(os.path.join(pro_dir, 'train.csv'))\n",
    "vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(pro_dir, 'validation_tr.csv'),\n",
    "                                           os.path.join(pro_dir, 'validation_te.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bigan:\n",
    "    def __init__(self, p_dims, d_dims,q_dims=None,random_seed=98765,g_weight = 0.5,lr=1e-3):\n",
    "        \n",
    "        self.p_dims = p_dims\n",
    "        self.d_dims = d_dims\n",
    "        self.random_seed = random_seed\n",
    "        self.lr = lr\n",
    "        self.g_weight = g_weight\n",
    "        if q_dims is None:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "        else:\n",
    "            assert q_dims[0] == p_dims[-1], \"Input and output dimension must equal each other for autoencoders.\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q-network mismatches.\"\n",
    "            \n",
    "        assert d_dims is not None, \"d_dims can't be None\"\n",
    "        assert d_dims[0] == self.p_dims[0] + self.q_dims[0], \"Shape mismatch: discriminate network\\\n",
    "        should be equal to the sum of input shape and output shape of p.\"\n",
    "            \n",
    "        self._construct_weights()\n",
    "        self.construct_placeholders()\n",
    "    def construct_placeholders(self):        \n",
    "        self.input_X = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.q_dims[0]])\n",
    "        self.input_z = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.p_dims[0]])\n",
    "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=None)\n",
    "        self.is_training_ph = tf.placeholder_with_default(0.0, shape=None)\n",
    "    \n",
    "    def _construct_weights(self):\n",
    "        self.weights_q, self.biases_q = [], []\n",
    "        \n",
    "        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1], self.q_dims[1:])):\n",
    "            weight_key = \"weight_q_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_q_{}\".format(i+1)\n",
    "            \n",
    "            self.weights_q.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_q.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # add summary stats\n",
    "            #tf.summary.histogram(weight_key, self.weights_q[-1])\n",
    "            #tf.summary.histogram(bias_key, self.biases_q[-1])\n",
    "            \n",
    "        self.weights_p, self.biases_p = [], []\n",
    "        \n",
    "        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-1], self.p_dims[1:])):\n",
    "            weight_key = \"weight_p_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_p_{}\".format(i+1)\n",
    "            self.weights_p.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_p.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "        self.weights_d,self.biases_d = [],[]\n",
    "        \n",
    "        for i, (d_in, d_out) in enumerate(zip(self.d_dims[:-1], self.d_dims[1:])):\n",
    "            weight_key = \"weight_d_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_d_{}\".format(i+1)\n",
    "            self.weights_d.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_d.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "        weight_key = \"weight_d_out\"\n",
    "        bias_key = \"bias_d_out\"\n",
    "        self.weights_d.append(tf.get_variable(\n",
    "                name=weight_key, shape=[self.d_dims[-1], 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "        self.biases_d.append(tf.get_variable(\n",
    "                name=bias_key, shape=[1],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "    def p_graph(self, z):\n",
    "        h = z\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_p, self.biases_p)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights_p) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return h\n",
    "    def q_graph(self,x):\n",
    "        \n",
    "        \n",
    "        h = tf.nn.l2_normalize(x, 1)\n",
    "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_q, self.biases_q)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            h = tf.nn.tanh(h)\n",
    "        return h\n",
    "    def forward(self,x):\n",
    "        z = self.q_graph(x)\n",
    "        x_hat = self.p_graph(z)\n",
    "        return x_hat\n",
    "    def d_graph(self,x,z):\n",
    "        concat = tf.concat([x,z],axis=1)\n",
    "        h = concat\n",
    "        # adding the random noise to make it harder for disriminator to do its job\n",
    "        #randNoise = tf.random_normal(tf.shape(h)) \n",
    "        #h += randNoise\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_d, self.biases_d)):\n",
    "            if i != len(self.weights_d):\n",
    "                h = tf.matmul(h, w) + b\n",
    "                h = tf.nn.tanh(h)\n",
    "            else:\n",
    "                h = tf.matmul(h,w) + b\n",
    "        logits = h\n",
    "        #return tf.nn.softmax(logits,axis=1)\n",
    "        return logits\n",
    "        \n",
    "    '''def cost(self,x,z_hat,z,x_hat):\n",
    "        def log(tensor):\n",
    "            return tf.log(tensor + 1e-8)\n",
    "        pred_q = self.d_graph(x,z_hat)\n",
    "        pred_p = self.d_graph(x_hat,z)\n",
    "        \n",
    "        #D(x,E(x))\n",
    "        sig_q = tf.nn.sigmoid(pred_q)\n",
    "        \n",
    "        #D(G(z),z)\n",
    "        sig_p = tf.nn.sigmoid(pred_p)\n",
    "        \n",
    "        # log(D(x,E(x))) encoder is true\n",
    "        loss1 = -tf.reduce_mean(log(sig_q))\n",
    "        \n",
    "        # log(1 - D(x,E(x))) encoder is false\n",
    "        loss2 = -tf.reduce_mean(log(1-sig_q))\n",
    "        \n",
    "        \n",
    "        # log(1 - D(G(z),z)) generator is false\n",
    "        loss3 = -tf.reduce_mean(log(1-sig_p))\n",
    "        \n",
    "        # log(D(G(z),z)) generator is true\n",
    "        \n",
    "        loss4 = -tf.reduce_mean(log(sig_p))\n",
    "        \n",
    "        \n",
    "        loss_d =  loss1 + loss3 \n",
    "        \n",
    "        #G is trying to fool the D, thus generator is approaching true and encoder is approaching false\n",
    "        loss_eg = loss2 + loss4\n",
    "        \n",
    "        para_eg = self.weights_q + self.weights_p + self.biases_q + self.biases_p\n",
    "        para_d = self.weights_d + self.biases_d\n",
    "        \n",
    "        \"\"\"\n",
    "        grad_eg = tf.gradients(self.g_weight * loss_eg, para_eg)\n",
    "        grad_d = tf.gradients((1 - self.g_weight)*loss_d, para_d)\n",
    "        \n",
    "        train_op = tf.train.AdamOptimizer(self.lr)\n",
    "        \n",
    "        train_op = train_op.apply_gradients(zip(grad_eg + grad_d, para_eg + para_d))\n",
    "        \"\"\"\n",
    "        EG_solver = (tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "            .minimize(loss_eg, var_list=para_eg))\n",
    "        D_solver = (tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "            .minimize(loss_d, var_list=para_d))\n",
    "        #tf.summary.scalar(\"loss1\",loss1)\n",
    "        #tf.summary.scalar(\"loss2\",loss2)\n",
    "        #tf.summary.scalar(\"loss3\",loss3)\n",
    "        #tf.summary.scalar(\"loss4\",loss4)\n",
    "        tf.summary.scalar(\"loss_eg\",loss_eg)\n",
    "        tf.summary.scalar(\"loss_d\", loss_d)\n",
    "        tf.summary.scalar(\"D_E_accuracy\", tf.reduce_mean(tf.cast(sig_q > 0.5, tf.float32)))\n",
    "        tf.summary.scalar(\"D_G_accuracy\", tf.reduce_mean(tf.cast(sig_p < 0.5, tf.float32)))\n",
    "        merged = tf.summary.merge_all()\n",
    "        return loss_eg, loss_d, EG_solver, D_solver, merged'''\n",
    "    \n",
    "    \n",
    "    def cost(self,x,z_hat,z,x_hat):\n",
    "        \n",
    "        # critic for encoder\n",
    "        pred_q = self.d_graph(x,z_hat)\n",
    "        \n",
    "        # critic for decoder\n",
    "        pred_p = self.d_graph(x_hat,z)\n",
    "        \n",
    "        #D(x,E(x))\n",
    "        sig_q = tf.nn.sigmoid(pred_q)\n",
    "        \n",
    "        #D(G(z),z)\n",
    "        sig_p = tf.nn.sigmoid(pred_p)\n",
    "        \n",
    "        # loss for D is decode - encoder this value should be negative in this case minimizing the negative value\n",
    "        # \n",
    "        loss_d = tf.reduce_mean(pred_p - pred_q)\n",
    "        \n",
    "        # loss for G and E \n",
    "        loss_eg = tf.reduce_mean(pred_q - pred_p)\n",
    "        \n",
    "        \n",
    "        para_eg = self.weights_q + self.weights_p + self.biases_q + self.biases_p\n",
    "        para_d = self.weights_d + self.biases_d\n",
    "        \n",
    "        EG_solver = (tf.train.RMSPropOptimizer(learning_rate=self.lr)\n",
    "            .minimize(loss_eg, var_list=para_eg))\n",
    "        D_solver = (tf.train.RMSPropOptimizer(learning_rate=self.lr)\n",
    "            .minimize(loss_d, var_list=para_d))\n",
    "        \n",
    "        clip_D = [p.assign(tf.clip_by_value(p, -0.02, 0.02)) for p in para_d]\n",
    "        #tf.summary.scalar(\"loss1\",loss1)\n",
    "        #tf.summary.scalar(\"loss2\",loss2)\n",
    "        #tf.summary.scalar(\"loss3\",loss3)\n",
    "        #tf.summary.scalar(\"loss4\",loss4)\n",
    "        tf.summary.scalar(\"loss_eg\",loss_eg)\n",
    "        tf.summary.scalar(\"loss_d\", loss_d)\n",
    "        tf.summary.scalar(\"D_E_accuracy\", tf.reduce_mean(tf.cast(sig_q > 0.5, tf.float32)))\n",
    "        tf.summary.scalar(\"D_G_accuracy\", tf.reduce_mean(tf.cast(sig_p < 0.5, tf.float32)))\n",
    "        merged = tf.summary.merge_all()\n",
    "        return loss_eg, loss_d, EG_solver, D_solver, clip_D, merged\n",
    "    \n",
    "    \n",
    "    def build_graph(self):\n",
    "        z_hat = self.q_graph(self.input_X)\n",
    "        z = tf.random_normal(tf.shape(z_hat))\n",
    "        x_hat = self.p_graph(self.is_training_ph * z + (1- self.is_training_ph) * z_hat)\n",
    "        loss_eg, loss_d, EG_solver, D_solver, clip_D ,merged = self.cost(self.input_X,z_hat,z,x_hat)\n",
    "        saver = tf.train.Saver()\n",
    "        return saver,x_hat,loss_eg, loss_d, EG_solver, clip_D , D_solver, merged\n",
    "        \n",
    "    def train(self,train,val_tr = None,val_te = None,n_epochs = 200,batch_size = 500):\n",
    "        #pass\n",
    "        ndcgs_vad = []\n",
    "        global predict_vad \n",
    "        N = train.shape[0]\n",
    "        idxlist = list(range(N))\n",
    "        saver, x_hat, loss_eg, loss_d, EG_solver, clip_D ,D_solver, merged = self.build_graph()\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            training_writer = tf.summary.FileWriter('./train_summary/g_{}/'.format(self.g_weight),\n",
    "                                      sess.graph)\n",
    "            sess.run(init)\n",
    "            count = 0\n",
    "            for epoch in range(n_epochs):\n",
    "                np.random.shuffle(idxlist)\n",
    "                start_idxs = list(range(0,N,batch_size))\n",
    "                end_idxs = start_idxs[1:] + [N]\n",
    "                for batch_num in range(len(start_idxs)):\n",
    "                    start_idx = start_idxs[batch_num]\n",
    "                    end_idx = end_idxs[batch_num]\n",
    "                    \n",
    "                    X = train[idxlist[start_idx:end_idx]]\n",
    "                    \n",
    "                    if sparse.isspmatrix(X):\n",
    "                        X = X.toarray()\n",
    "                    X = X.astype(\"float32\")\n",
    "                    \n",
    "                    feed_dict = {self.input_X:X,\n",
    "                                 self.keep_prob_ph:0.5,\n",
    "                                 self.is_training_ph:1}\n",
    "                    \n",
    "                    for _ in range(5): \n",
    "                        d_batch = np.random.randint(len(start_idxs))\n",
    "                        D_X = train[idxlist[start_idxs[d_batch]:end_idxs[d_batch]]]\n",
    "                        if sparse.isspmatrix(D_X):\n",
    "                            D_X = D_X.toarray()\n",
    "                            \n",
    "                        D_X = D_X.astype(\"float32\")\n",
    "                        \n",
    "                        feed_D = {self.input_X:D_X,\n",
    "                                 self.keep_prob_ph:0.5,\n",
    "                                 self.is_training_ph:1}\n",
    "                        #print (feed_D)\n",
    "                        sess.run(D_solver,feed_dict=feed_D)\n",
    "                        sess.run(clip_D,feed_dict=feed_D)\n",
    "                    sess.run(EG_solver, feed_dict=feed_dict)\n",
    "                    if batch_num % 100 == 0:\n",
    "                        #summary_train = sess.run(merged,feed_dict = feed_dict)\n",
    "                        merged_summary = sess.run(merged, feed_dict = feed_dict)\n",
    "                        training_writer.add_summary(merged_summary, global_step=epoch*int(N/batch_size/100) + batch_num)\n",
    "                        if val_tr != None and val_te != None:\n",
    "                            val_predict = self.predict(val_tr,sess,x_hat)\n",
    "                            #predict_vad.append(val_predict)\n",
    "                            #print (val_predict)\n",
    "                            val_predict[val_tr.nonzero()] = -np.inf\n",
    "                            ndcg_dist = np.array(NDCG_binary_at_k_batch(val_predict,val_te))\n",
    "                            ndcg_val = ndcg_dist.mean()\n",
    "                            #print (\"Validation of NDCG @ 100 at epoch {} is {}\".format(epoch,ndcg_val))\n",
    "                            ndcgs_vad.append(ndcg_val)\n",
    "                    count += 1\n",
    "        return ndcgs_vad          \n",
    "    def predict(self,test,sess,x_hat,batch_size = 500):\n",
    "        \n",
    "        N = test.shape[0]\n",
    "        res = []\n",
    "        for start_idx in range(0,N,batch_size):\n",
    "            end_idx = min(start_idx + batch_size, N)\n",
    "            X = test[start_idx:end_idx]\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype(\"float32\")\n",
    "            feed_dict = {self.input_X:X,\n",
    "                         self.is_training_ph:0}\n",
    "            res.append(sess.run(x_hat,feed_dict=feed_dict))\n",
    "        return np.vstack(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_vad = []\n",
    "tf.reset_default_graph()\n",
    "bg = bigan([200,600,n_items],[n_items+200,100],random_seed=98765,g_weight = .5, lr=2e-4)\n",
    "ndcgs_vad = bg.train(train_data,vad_data_tr,vad_data_te,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
